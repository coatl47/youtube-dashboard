import streamlit as st
from googleapiclient.discovery import build
import pandas as pd
import plotly.express as px
import google.generativeai as genai
import re
import io
from datetime import datetime

# [1] ì„¤ì • ë° API ì—°ê²°
# Streamlit Secretsì— YOUTUBE_API_KEYì™€ GEMINI_API_KEYê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
Y_KEY = st.secrets["YOUTUBE_API_KEY"]
G_KEY = st.secrets["GEMINI_API_KEY"]

# API ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
youtube = build('youtube', 'v3', developerKey=Y_KEY)
genai.configure(api_key=G_KEY)

# [2] ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ (ê¸°ëŠ¥ë³„ ë¶„ë¦¬)
@st.cache_data(ttl=600)
def get_video_info(v_id):
    try:
        r = youtube.videos().list(part="snippet,statistics", id=v_id).execute()
        item = r['items'][0]
        return {
            "title": item['snippet']['title'],
            "views": int(item['statistics']['viewCount']),
            "likes": int(item['statistics']['likeCount']),
            "comms": int(item['statistics']['commentCount'])
        }
    except: return None

def get_comments_data(v_id, limit=30):
    comments = []
    try:
        r = youtube.commentThreads().list(part="snippet", videoId=v_id, maxResults=50, order="time").execute()
        for item in r['items']:
            txt = item['snippet']['topLevelComment']['snippet']['textDisplay']
            time = item['snippet']['topLevelComment']['snippet']['publishedAt']
            clean_txt = re.sub('<[^<]+?>', '', txt).replace('\n', ' ')
            comments.append({"time": time, "text": clean_txt})
            if len(comments) >= limit: break
        return pd.DataFrame(comments)
    except: return pd.DataFrame()

# [3] AI ë¶„ì„ í•¨ìˆ˜ (í‘œì¤€ í˜¸ì¶œ ë°©ì‹ ì‚¬ìš©)
def analyze_with_gemini(df):
    if df.empty: return pd.DataFrame()
    
    # ëª¨ë¸ ì„ ì–¸ (ê°€ì¥ í‘œì¤€ì ì¸ ë°©ì‹)
    model = genai.GenerativeModel('gemini-1.5-flash')
    
    raw_text = "\n".join([f"- {t[:100]}" for t in df['text']])
    prompt = f"""
    ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ ê²°ê³¼ë¥¼ '|' êµ¬ë¶„ìì˜ CSV í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•´ì¤˜.
    ë¶„ë¥˜(ì£¼ì œ)ëŠ” ìµœëŒ€ 9ê°œ ì´ë‚´ë¡œ ìƒì„±í•´.
    í˜•ì‹: ê°ì„±|ë¶„ë¥˜|í‚¤ì›Œë“œ|ë‚´ìš©
    (ê°ì„±: ê¸ì •, ì¤‘ë¦½, ë¶€ì • ì¤‘ í•˜ë‚˜)
    
    ëŒ“ê¸€ ëª©ë¡:
    {raw_text}
    """
    
    try:
        response = model.generate_content(prompt)
        res_txt = response.text.strip()
        
        # ë¶ˆí•„ìš”í•œ ë§ˆí¬ë‹¤ìš´ ì œê±°
        clean_csv = re.sub(r'```csv\n|```', '', res_txt)
        if "ê°ì„±|ë¶„ë¥˜" in clean_csv:
            start = clean_csv.find("ê°ì„±|ë¶„ë¥˜")
            final_df = pd.read_csv(io.StringIO(clean_csv[start:]), sep='|', on_bad_lines='skip', engine='python')
            final_df.columns = [c.strip() for c in final_df.columns]
            return final_df
        return pd.DataFrame()
    except Exception as e:
        st.error(f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame()

# [4] ëŒ€ì‹œë³´ë“œ UI êµ¬ì„±
st.set_page_config(page_title="ìœ íŠœë¸Œ ëª¨ë‹ˆí„°ë§", layout="wide")
st.title("ğŸ“Š ìœ íŠœë¸Œ ì‹¤ì‹œê°„ ì—¬ë¡  ë¶„ì„ ëŒ€ì‹œë³´ë“œ")

video_url = st.text_input("ìœ íŠœë¸Œ URLì„ ì…ë ¥í•˜ì„¸ìš”")

if video_url:
    match = re.search(r'(?:v=|\/)([0-9A-Za-z_-]{11}).*', video_url)
    if match:
        vid = match.group(1)
        
        with st.status("ë¶„ì„ ì¤‘...", expanded=True) as status:
            info = get_video_info(vid)
            raw = get_comments_data(vid)
            final = analyze_with_gemini(raw)
            status.update(label="ì²˜ë¦¬ ì™„ë£Œ!", state="complete", expanded=False)

        if info and not final.empty:
            st.divider()
            st.subheader(f"ğŸ¥ {info['title']}")
            
            # ì§€í‘œ ì˜ì—­
            m1, m2, m3, m4 = st.columns(4)
            m1.metric("ì¡°íšŒìˆ˜", f"{info['views']:,}")
            m2.metric("ì¢‹ì•„ìš”", f"{info['likes']:,}")
            m3.metric("ëŒ“ê¸€ìˆ˜", f"{info['comms']:,}")
            m4.metric("ë¶„ì„ê¸°ì¤€", datetime.now().strftime('%Y-%m-%d'))

            # ì°¨íŠ¸ ì˜ì—­
            c1, c2 = st.columns(2)
            with c1:
                st.subheader("ğŸ“ˆ ì‹œê°„ëŒ€ë³„ ëŒ“ê¸€ ì¶”ì´")
                raw['time'] = pd.to_datetime(raw['time'])
                trend = raw.set_index('time').resample('H').size().reset_index(name='cnt')
                st.plotly_chart(px.line(trend, x='time', y='cnt', markers=True), use_container_width=True)
            with c2:
                st.subheader("ğŸ˜Š ê°ì„± ë¶„ì„")
                s_data = final['ê°ì„±'].value_counts().reset_index()
                st.plotly_chart(px.pie(s_data, names='ê°ì„±', values='count', 
                                       color='ê°ì„±', color_discrete_map={'ê¸ì •':'#00CC96','ë¶€ì •':'#EF553B','ì¤‘ë¦½':'#AB63FA'}), use_container_width=True)

            # ë¶„ë¥˜ë³„ ë¶„ì„
            st.subheader("ğŸ“ ì£¼ì œë³„ ì—¬ë¡  ë¶„í¬ (ìµœëŒ€ 9ê°œ)")
            b_data = final.groupby(['ë¶„ë¥˜', 'ê°ì„±']).size().reset_index(name='v')
            st.plotly_chart(px.bar(b_data, x='v', y='ë¶„ë¥˜', color='ê°ì„±', orientation='h',
                                   color_discrete_map={'ê¸ì •':'#00CC96','ë¶€ì •':'#EF553B','ì¤‘ë¦½':'#AB63FA'}), use_container_width=True)

            # ë°ì´í„° í…Œì´ë¸”
            st.subheader("ğŸ“‹ ë¶„ì„ ìƒì„¸ ë°ì´í„°")
            st.dataframe(final, use_container_width=True)
            st.download_button("ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ", final.to_csv(index=False).encode('utf-8-sig'), "result.csv")
        elif info:
            st.warning("ë°ì´í„°ëŠ” ê°€ì ¸ì™”ìœ¼ë‚˜ AIê°€ ë¶„ì„ ê²°ê³¼ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ë³´ì„¸ìš”.")
    else:
        st.error("ì˜¬ë°”ë¥¸ ìœ íŠœë¸Œ ì£¼ì†Œê°€ ì•„ë‹™ë‹ˆë‹¤.")
