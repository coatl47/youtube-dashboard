import streamlit as st
from googleapiclient.discovery import build
import pandas as pd
import plotly.express as px
import google.generativeai as genai
import re
import io
from datetime import datetime

# 1. API ì„¤ì •
API_KEY = st.secrets["YOUTUBE_API_KEY"]
GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]

youtube = build('youtube', 'v3', developerKey=API_KEY)
genai.configure(api_key=GEMINI_API_KEY)

# [ìˆ˜ì • í¬ì¸íŠ¸] ëª¨ë¸ ê²½ë¡œë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•˜ì—¬ 404 ì—ëŸ¬ ë°©ì§€
model = genai.GenerativeModel('models/gemini-1.5-flash')

# 2. ìˆ˜ì§‘ í•¨ìˆ˜
@st.cache_data(ttl=600)
def get_stats(v_id):
    try:
        r = youtube.videos().list(part="snippet,statistics", id=v_id).execute()
        return {
            "title": r['items'][0]['snippet']['title'],
            "v_count": int(r['items'][0]['statistics']['viewCount']),
            "l_count": int(r['items'][0]['statistics']['likeCount']),
            "c_count": int(r['items'][0]['statistics']['commentCount'])
        }
    except: return None

def get_comms(v_id, limit=30):
    comms = []
    try:
        r = youtube.commentThreads().list(part="snippet", videoId=v_id, maxResults=50, order="time").execute()
        for item in r['items']:
            snip = item['snippet']['topLevelComment']['snippet']
            comms.append({"time": snip['publishedAt'], "text": snip['textDisplay']})
            if len(comms) >= limit: break
        return pd.DataFrame(comms)
    except: return pd.DataFrame()

# 3. AI ë¶„ì„ í•¨ìˆ˜ (404 ëŒ€ì‘ ë° íŒŒì‹± ê°•í™”)
def analyze_ai(df):
    if df.empty: return pd.DataFrame()
    # ëŒ“ê¸€ í…ìŠ¤íŠ¸ ì •ì œ (HTML íƒœê·¸ ì œê±° ë° ê¸¸ì´ ì œí•œ)
    raw_txt = "\n".join([f"- {re.sub('<[^<]+?>', '', t)[:100]}" for t in df['text']])
    
    prompt = f"""
    ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì¤˜.
    
    [ì‘ì—… ì§€ì¹¨]
    1. ì£¼ì œ(ë¶„ë¥˜)ë¥¼ ì˜ìƒ ë‚´ìš©ì— ë§ê²Œ ì§ì ‘ ìƒì„±í•´ (ìµœëŒ€ 9ê°œ).
    2. ëª¨ë“  ëŒ“ê¸€ì„ [ê°ì„±, ë¶„ë¥˜, í‚¤ì›Œë“œ, ë‚´ìš©]ìœ¼ë¡œ ë¶„ë¥˜í•´.
    3. ê²°ê³¼ëŠ” ë°˜ë“œì‹œ '|' êµ¬ë¶„ìë¥¼ ì‚¬ìš©í•œ CSV í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•´.
    4. CSV í—¤ë”ëŠ” 'ê°ì„±|ë¶„ë¥˜|í‚¤ì›Œë“œ|ë‚´ìš©' ì´ì–´ì•¼ í•´.
    5. ì„œë¡ ì´ë‚˜ ê²°ë¡  ê°™ì€ ë¶€ê°€ ì„¤ëª…ì€ ì ˆëŒ€ í•˜ì§€ ë§ˆ.
    
    ëŒ“ê¸€ ëª©ë¡:
    {raw_txt}
    """
    try:
        # ëª¨ë¸ í˜¸ì¶œ
        response = model.generate_content(prompt)
        full_text = response.text.strip()
        
        # ë°ì´í„°í”„ë ˆì„ ë³€í™˜ ë¡œì§
        if "ê°ì„±|ë¶„ë¥˜" in full_text:
            start_idx = full_text.find("ê°ì„±|ë¶„ë¥˜")
            clean_csv = full_text[start_idx:].strip().replace("```csv", "").replace("```", "")
            
            rdf = pd.read_csv(io.StringIO(clean_csv), sep='|', on_bad_lines='skip', engine='python')
            rdf.columns = [c.strip() for c in rdf.columns]
            return rdf
        else:
            return pd.DataFrame()
    except Exception as e:
        # ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ì‚¬ìš©ìì—ê²Œ êµ¬ì²´ì ìœ¼ë¡œ í‘œì¶œ
        st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame()

# 4. UI êµ¬ì„±
st.set_page_config(page_title="ìœ íŠœë¸Œ ì—¬ë¡  ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ“Š ìœ íŠœë¸Œ ì‹¤ì‹œê°„ ì—¬ë¡  ë¶„ì„ ëŒ€ì‹œë³´ë“œ")

url = st.text_input("ìœ íŠœë¸Œ URL ì…ë ¥ (ì˜ˆ: https://www.youtube.com/watch?v=...)", key="yt_url")

if url:
    m = re.search(r'(?:v=|\/)([0-9A-Za-z_-]{11}).*', url)
    if m:
        vid = m.group(1)
        
        with st.status("ë¶„ì„ ë°ì´í„°ë¥¼ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤...", expanded=True) as status:
            info = get_stats(vid)
            raw = get_comms(vid)
            final = analyze_ai(raw)
            if not final.empty:
                status.update(label="ë¶„ì„ ì™„ë£Œ!", state="complete", expanded=False)
            else:
                status.update(label="ë¶„ì„ ì‹¤íŒ¨", state="error", expanded=False)

        if info and not final.empty:
            st.divider()
            st.subheader(f"ğŸ¥ ë¶„ì„ ì˜ìƒ: {info['title']}")
            
            # ì§€í‘œ í‘œì‹œ
            i1, i2, i3, i4 = st.columns(4)
            i1.metric("ì¡°íšŒìˆ˜", f"{info['v_count']:,}")
            i2.metric("ì¢‹ì•„ìš”", f"{info['l_count']:,}")
            i3.metric("ëŒ“ê¸€ìˆ˜", f"{info['c_count']:,}")
            i4.metric("ì—…ë°ì´íŠ¸", datetime.now().strftime('%H:%M'))

            # ì°¨íŠ¸
            c1, c2 = st.columns(2)
            with c1:
                st.subheader("ğŸ“ˆ ì‹œê°„ëŒ€ë³„ ëŒ“ê¸€ ì¶”ì´")
                raw['time'] = pd.to_datetime(raw['time'])
                trend = raw.set_index('time').resample('H').size().reset_index(name='cnt')
                st.plotly_chart(px.line(trend, x='time', y='cnt', markers=True), use_container_width=True)
            with c2:
                st.subheader("ğŸ˜Š ê°ì„± ë¶„í¬")
                s_counts = final['ê°ì„±'].value_counts().reset_index()
                st.plotly_chart(px.pie(s_counts, names='ê°ì„±', values='count', 
                                       color='ê°ì„±', color_discrete_map={'ê¸ì •':'#00CC96','ë¶€ì •':'#EF553B','ì¤‘ë¦½':'#AB63FA'}), use_container_width=True)

            # ê°€ë¡œ ë§‰ëŒ€ ê·¸ë˜í”„
            st.subheader("ğŸ“ ì£¼ì œë³„ ì—¬ë¡  ë¶„ì„ (AI ìë™ ìƒì„±)")
            b_data = final.groupby(['ë¶„ë¥˜', 'ê°ì„±']).size().reset_index(name='v')
            st.plotly_chart(px.bar(b_data, x='v', y='ë¶„ë¥˜', color='ê°ì„±', orientation='h',
                                   color_discrete_map={'ê¸ì •':'#00CC96','ë¶€ì •':'#EF553B','ì¤‘ë¦½':'#AB63FA'}), use_container_width=True)

            # ë°ì´í„° í…Œì´ë¸”
            st.subheader("ğŸ“‹ ìƒì„¸ ë¶„ì„ ë°ì´í„°")
            st.dataframe(final, use_container_width=True, height=400)
            st.download_button("ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ", final.to_csv(index=False).encode('utf-8-sig'), f"analysis_{vid}.csv", "text/csv")
        elif info:
            st.warning("ë¶„ì„ ê²°ê³¼ ë°ì´í„°ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
    else:
        st.error("ìœ íš¨í•œ ìœ íŠœë¸Œ ì£¼ì†Œê°€ ì•„ë‹™ë‹ˆë‹¤.")
